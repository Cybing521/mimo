"""
Neural Network Architectures for PPO Agent
===========================================

Actor-Critic networks with support for continuous action spaces.
å®žçŽ°äº†æ–¹æ¡ˆ7ï¼šæ”¹è¿›ç½‘ç»œæž¶æž„ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ + æ®‹å·®è¿žæŽ¥ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple
import numpy as np


class ResidualBlock(nn.Module):
    """
    æ®‹å·®è¿žæŽ¥å—ï¼ˆæ–¹æ¡ˆ7.2ï¼‰
    æå‡ç½‘ç»œæ·±åº¦å’Œè®­ç»ƒç¨³å®šæ€§
    """
    def __init__(self, dim: int, dropout: float = 0.1):
        super(ResidualBlock, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(dim, dim),
            nn.LayerNorm(dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim, dim),
            nn.LayerNorm(dim),
        )
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # æ®‹å·®è¿žæŽ¥ + Dropout
        out = self.layers(x)
        return F.relu(x + self.dropout(out))


class SelfAttentionLayer(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›å±‚ï¼ˆæ–¹æ¡ˆ7.1ï¼‰
    å¸®åŠ©ç½‘ç»œå…³æ³¨çŠ¶æ€ä¸­çš„é‡è¦ç‰¹å¾
    """
    def __init__(self, dim: int, num_heads: int = 4, dropout: float = 0.1):
        super(SelfAttentionLayer, self).__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        
        assert dim % num_heads == 0, f"dim {dim} must be divisible by num_heads {num_heads}"
        
        # ä½¿ç”¨çº¿æ€§å±‚å®žçŽ°å¤šå¤´æ³¨æ„åŠ›ï¼ˆé¿å…åºåˆ—ç»´åº¦é—®é¢˜ï¼‰
        self.query = nn.Linear(dim, dim)
        self.key = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        self.output = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch_size, dim) - çŠ¶æ€å‘é‡
        Returns:
            out: (batch_size, dim) - æ³¨æ„åŠ›å¢žå¼ºåŽçš„ç‰¹å¾
        """
        # å°†çŠ¶æ€å‘é‡æ‰©å±•ä¸ºåºåˆ—ï¼ˆbatch_size, 1, dimï¼‰
        x_expanded = x.unsqueeze(1)  # (batch, 1, dim)
        
        # è®¡ç®— Q, K, V
        Q = self.query(x_expanded)  # (batch, 1, dim)
        K = self.key(x_expanded)
        V = self.value(x_expanded)
        
        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼ (batch, 1, num_heads, head_dim)
        batch_size = x.size(0)
        Q = Q.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, num_heads, 1, head_dim)
        K = K.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)  # (batch, num_heads, 1, 1)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attn_output = torch.matmul(attn_weights, V)  # (batch, num_heads, 1, head_dim)
        attn_output = attn_output.transpose(1, 2).contiguous()  # (batch, 1, num_heads, head_dim)
        attn_output = attn_output.view(batch_size, 1, self.dim)  # (batch, 1, dim)
        
        # è¾“å‡ºæŠ•å½±
        out = self.output(attn_output)  # (batch, 1, dim)
        out = self.dropout(out)
        
        # æ®‹å·®è¿žæŽ¥ + LayerNorm
        out = self.layer_norm(x_expanded + out)  # (batch, 1, dim)
        
        # åŽ‹ç¼©å›ž (batch, dim)
        return out.squeeze(1)


class ActorNetwork(nn.Module):
    """
    Actor Network (Policy Network)
    
    Outputs Gaussian policy: Ï€(a|s) = N(Î¼(s), Ïƒ(s))
    
    ðŸ“Œ å¯¹åº”ç›´è§‰ï¼šç»™å®šâ€œå½“å‰ä¿¡é“+å¤©çº¿ä½ç½®â€ï¼Œç½‘ç»œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œç»´åº¦çš„
    å¹³å‡å€¼ä¸Žæ–¹å·®ï¼›é‡‡æ ·åŽçš„å¢žé‡ä¼šè¢«çŽ¯å¢ƒå†ç¼©æ”¾åˆ° Â±0.1Î»ã€‚
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dims: Tuple[int, ...] = (512, 256, 128),
        log_std_min: float = -20,
        log_std_max: float = 2,
        use_attention: bool = True,
        use_residual: bool = True,
        dropout: float = 0.1,
    ):
        """
        Initialize Actor Network with improved architecture (æ–¹æ¡ˆ7)
        
        Args:
            state_dim: Dimension of state space
            action_dim: Dimension of action space
            hidden_dims: Tuple of hidden layer dimensions
            log_std_min: Minimum log standard deviation
            log_std_max: Maximum log standard deviation
            use_attention: Whether to use self-attention (æ–¹æ¡ˆ7.1)
            use_residual: Whether to use residual blocks (æ–¹æ¡ˆ7.2)
            dropout: Dropout rate for regularization
        """
        super(ActorNetwork, self).__init__()
        
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.use_attention = use_attention
        self.use_residual = use_residual
        
        # è¾“å…¥å±‚
        self.input_layer = nn.Sequential(
            nn.Linear(state_dim, hidden_dims[0]),
            nn.LayerNorm(hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        
        # è‡ªæ³¨æ„åŠ›å±‚ï¼ˆæ–¹æ¡ˆ7.1ï¼‰
        if use_attention and hidden_dims[0] % 4 == 0:
            self.attention = SelfAttentionLayer(hidden_dims[0], num_heads=4, dropout=dropout)
        else:
            self.attention = None
            if use_attention:
                print(f"âš ï¸  è­¦å‘Š: hidden_dim={hidden_dims[0]} ä¸èƒ½è¢«4æ•´é™¤ï¼Œè·³è¿‡æ³¨æ„åŠ›å±‚")
        
        # å…±äº«å±‚ï¼ˆå¸¦æ®‹å·®è¿žæŽ¥ï¼‰
        shared_layers = []
        prev_dim = hidden_dims[0]
        
        for i, hidden_dim in enumerate(hidden_dims[1:], 1):
            # æ·»åŠ æ®‹å·®å—ï¼ˆæ–¹æ¡ˆ7.2ï¼‰
            if use_residual and prev_dim == hidden_dim:
                shared_layers.append(ResidualBlock(prev_dim, dropout=dropout))
            else:
                # æ™®é€šå±‚ï¼ˆç»´åº¦å˜åŒ–æ—¶ä¸èƒ½ä½¿ç”¨æ®‹å·®ï¼‰
                shared_layers.append(nn.Linear(prev_dim, hidden_dim))
                shared_layers.append(nn.LayerNorm(hidden_dim))
                shared_layers.append(nn.ReLU())
                shared_layers.append(nn.Dropout(dropout))
                prev_dim = hidden_dim
        
        self.shared = nn.Sequential(*shared_layers)
        
        # Mean headï¼ˆtanh ä¿è¯å‡å€¼ âˆˆ [-1, 1]ï¼‰
        self.mean = nn.Sequential(
            nn.Linear(prev_dim, action_dim),
            nn.Tanh()  # Limit action range
        )
        
        # Log std head (learnable, æ·»åŠ æ•°å€¼ç¨³å®šæ€§)
        self.log_std = nn.Sequential(
            nn.Linear(prev_dim, action_dim),
        )
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass with NaN detection and numerical stability
        
        Args:
            state: State tensor (batch_size, state_dim)
        
        Returns:
            mean: Action mean (batch_size, action_dim)
            std: Action standard deviation (batch_size, action_dim)
        """
        # NaNæ£€æµ‹ï¼šæ£€æŸ¥è¾“å…¥
        if torch.isnan(state).any() or torch.isinf(state).any():
            print("âš ï¸  è­¦å‘Š: è¾“å…¥stateåŒ…å«NaNæˆ–Infï¼Œä½¿ç”¨é›¶å¡«å……")
            state = torch.where(torch.isnan(state) | torch.isinf(state), 
                              torch.zeros_like(state), state)
        
        # è¾“å…¥å±‚
        features = self.input_layer(state)
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆæ–¹æ¡ˆ7.1ï¼‰
        if self.attention is not None:
            features = self.attention(features)
        
        # å…±äº«å±‚
        features = self.shared(features)
        
        # NaNæ£€æµ‹ï¼šæ£€æŸ¥ä¸­é—´ç‰¹å¾
        if torch.isnan(features).any() or torch.isinf(features).any():
            print("âš ï¸  è­¦å‘Š: ä¸­é—´ç‰¹å¾åŒ…å«NaNæˆ–Infï¼Œä½¿ç”¨é›¶å¡«å……")
            features = torch.where(torch.isnan(features) | torch.isinf(features),
                                  torch.zeros_like(features), features)
        
        # Mean head
        mean = self.mean(features)
        
        # Log std headï¼ˆæ·»åŠ æ•°å€¼ç¨³å®šæ€§ï¼‰
        log_std = self.log_std(features)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std)
        
        # ç¡®ä¿stdä¸ä¼šå¤ªå°ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
        std = torch.clamp(std, min=1e-6)
        
        # æœ€ç»ˆNaNæ£€æµ‹
        if torch.isnan(mean).any() or torch.isnan(std).any():
            print("âš ï¸  è­¦å‘Š: è¾“å‡ºåŒ…å«NaNï¼Œä½¿ç”¨é»˜è®¤å€¼")
            mean = torch.zeros_like(mean)
            std = torch.ones_like(std) * 0.1
        
        return mean, std
    
    def get_action(
        self, 
        state: torch.Tensor, 
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Sample action from policy
        
        Args:
            state: State tensor
            deterministic: If True, return mean action
        
        Returns:
            action: Sampled action
            log_prob: Log probability of action
        """
        mean, std = self.forward(state)
        
        if deterministic:
            action = mean
            log_prob = None
        else:
            dist = torch.distributions.Normal(mean, std)  # è¿žç»­åŠ¨ä½œçš„å¸¸è§è®¾è®¡
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        
        return action, log_prob
    
    def evaluate_actions(
        self, 
        state: torch.Tensor, 
        action: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Evaluate log probability and entropy of given actions
        
        Args:
            state: State tensor
            action: Action tensor
        
        Returns:
            log_prob: Log probability of actions
            entropy: Policy entropy
        """
        mean, std = self.forward(state)
        dist = torch.distributions.Normal(mean, std)
        
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        entropy = dist.entropy().sum(dim=-1, keepdim=True)
        
        return log_prob, entropy


class CriticNetwork(nn.Module):
    """
    Critic Network (Value Network) with Dueling Architecture.
    å®žçŽ°äº†æ–¹æ¡ˆ7ï¼šæ”¹è¿›ç½‘ç»œæž¶æž„ï¼ˆæ®‹å·®è¿žæŽ¥ï¼‰
    
    Dueling ç»“æž„å¸¸è§äºŽå€¼å‡½æ•°ï¼Œç”¨"å…±äº«å¹²è·¯ + Value stream"æ¥èŽ·å¾—ä¸€ä¸ªæ ‡é‡ V(s)ã€‚
    åœ¨è¿žç»­åŠ¨ä½œçŽ¯å¢ƒä¸­ï¼Œè¿™æ ·çš„å†—ä½™å±‚å¯ä»¥æå‡ä¼°è®¡å¹³ç¨³åº¦ã€‚
    """
    
    def __init__(
        self,
        state_dim: int,
        hidden_dims: Tuple[int, ...] = (512, 256, 128),
        use_dueling: bool = True,
        use_residual: bool = True,
        dropout: float = 0.1,
    ):
        """
        Initialize Critic Network with improved architecture (æ–¹æ¡ˆ7)
        
        Args:
            state_dim: Dimension of state space
            hidden_dims: Tuple of hidden layer dimensions
            use_dueling: Whether to use dueling architecture
            use_residual: Whether to use residual blocks (æ–¹æ¡ˆ7.2)
            dropout: Dropout rate for regularization
        """
        super(CriticNetwork, self).__init__()
        
        self.use_dueling = use_dueling
        self.use_residual = use_residual
        
        # è¾“å…¥å±‚
        self.input_layer = nn.Sequential(
            nn.Linear(state_dim, hidden_dims[0]),
            nn.LayerNorm(hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        
        # Shared layersï¼ˆå¸¦æ®‹å·®è¿žæŽ¥ï¼‰
        shared_layers = []
        prev_dim = hidden_dims[0]
        
        for i, hidden_dim in enumerate(hidden_dims[1:-1], 1):  # All but last
            # æ·»åŠ æ®‹å·®å—ï¼ˆæ–¹æ¡ˆ7.2ï¼‰
            if use_residual and prev_dim == hidden_dim:
                shared_layers.append(ResidualBlock(prev_dim, dropout=dropout))
            else:
                # æ™®é€šå±‚ï¼ˆç»´åº¦å˜åŒ–æ—¶ä¸èƒ½ä½¿ç”¨æ®‹å·®ï¼‰
                shared_layers.append(nn.Linear(prev_dim, hidden_dim))
                shared_layers.append(nn.LayerNorm(hidden_dim))
                shared_layers.append(nn.ReLU())
                shared_layers.append(nn.Dropout(dropout))
                prev_dim = hidden_dim
        
        self.shared = nn.Sequential(*shared_layers)
        
        if use_dueling:
            # Dueling architecture
            last_hidden = hidden_dims[-1]
            
            # Value stream
            self.value_stream = nn.Sequential(
                nn.Linear(prev_dim, last_hidden),
                nn.LayerNorm(last_hidden),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(last_hidden, 1)
            )
        else:
            # Standard value head
            self.value_head = nn.Sequential(
                nn.Linear(prev_dim, hidden_dims[-1]),
                nn.LayerNorm(hidden_dims[-1]),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dims[-1], 1)
            )
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with NaN detection
        
        Args:
            state: State tensor (batch_size, state_dim)
        
        Returns:
            value: State value (batch_size, 1)
        """
        # NaNæ£€æµ‹ï¼šæ£€æŸ¥è¾“å…¥
        if torch.isnan(state).any() or torch.isinf(state).any():
            print("âš ï¸  è­¦å‘Š: Criticè¾“å…¥stateåŒ…å«NaNæˆ–Infï¼Œä½¿ç”¨é›¶å¡«å……")
            state = torch.where(torch.isnan(state) | torch.isinf(state),
                              torch.zeros_like(state), state)
        
        # è¾“å…¥å±‚
        features = self.input_layer(state)
        
        # å…±äº«å±‚
        features = self.shared(features)
        
        # NaNæ£€æµ‹ï¼šæ£€æŸ¥ä¸­é—´ç‰¹å¾
        if torch.isnan(features).any() or torch.isinf(features).any():
            print("âš ï¸  è­¦å‘Š: Criticä¸­é—´ç‰¹å¾åŒ…å«NaNæˆ–Infï¼Œä½¿ç”¨é›¶å¡«å……")
            features = torch.where(torch.isnan(features) | torch.isinf(features),
                                  torch.zeros_like(features), features)
        
        # Value head
        if self.use_dueling:
            value = self.value_stream(features)
        else:
            value = self.value_head(features)
        
        # æœ€ç»ˆNaNæ£€æµ‹
        if torch.isnan(value).any() or torch.isinf(value).any():
            print("âš ï¸  è­¦å‘Š: Criticè¾“å‡ºåŒ…å«NaNæˆ–Infï¼Œä½¿ç”¨é›¶å€¼")
            value = torch.zeros_like(value)
        
        return value


class ActorCritic(nn.Module):
    """
    Combined Actor-Critic Network with improved architecture (æ–¹æ¡ˆ7)
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        actor_hidden: Tuple[int, ...] = (512, 256, 128),
        critic_hidden: Tuple[int, ...] = (512, 256, 128),
        use_attention: bool = True,
        use_residual: bool = True,
        dropout: float = 0.1,
    ):
        """
        Initialize Actor-Critic with improved architecture (æ–¹æ¡ˆ7)
        
        Args:
            state_dim: Dimension of state space
            action_dim: Dimension of action space
            actor_hidden: Hidden dimensions for actor
            critic_hidden: Hidden dimensions for critic
            use_attention: Whether to use self-attention in actor (æ–¹æ¡ˆ7.1)
            use_residual: Whether to use residual blocks (æ–¹æ¡ˆ7.2)
            dropout: Dropout rate for regularization
        """
        super(ActorCritic, self).__init__()
        
        self.actor = ActorNetwork(
            state_dim, action_dim, actor_hidden,
            use_attention=use_attention,
            use_residual=use_residual,
            dropout=dropout,
        )
        self.critic = CriticNetwork(
            state_dim, critic_hidden,
            use_residual=use_residual,
            dropout=dropout,
        )
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through both networks
        
        Args:
            state: State tensor
        
        Returns:
            action: Sampled action
            value: State value
        """
        action, _ = self.actor.get_action(state)
        value = self.critic(state)
        return action, value
    
    def get_action_and_value(
        self, 
        state: torch.Tensor, 
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Get action, log probability, and value
        
        Args:
            state: State tensor
            deterministic: If True, use deterministic policy
        
        Returns:
            action: Selected action
            log_prob: Log probability of action
            value: State value
        """
        action, log_prob = self.actor.get_action(state, deterministic)
        value = self.critic(state)
        return action, log_prob, value


# Test networks
if __name__ == "__main__":
    print("Testing neural networks...")
    
    # Parameters
    state_dim = 20  # For N=4
    action_dim = 8  # 2*N
    batch_size = 32
    
    # Create networks
    actor = ActorNetwork(state_dim, action_dim)
    critic = CriticNetwork(state_dim)
    ac = ActorCritic(state_dim, action_dim)
    
    # Test data
    states = torch.randn(batch_size, state_dim)
    actions = torch.randn(batch_size, action_dim)
    
    # Test actor
    print("\n1. Testing Actor Network...")
    mean, std = actor(states)
    print(f"   Mean shape: {mean.shape}, Std shape: {std.shape}")
    
    action, log_prob = actor.get_action(states)
    print(f"   Action shape: {action.shape}, Log prob shape: {log_prob.shape}")
    
    log_prob_eval, entropy = actor.evaluate_actions(states, actions)
    print(f"   Evaluated log prob: {log_prob_eval.shape}, Entropy: {entropy.shape}")
    
    # Test critic
    print("\n2. Testing Critic Network...")
    values = critic(states)
    print(f"   Value shape: {values.shape}")
    
    # Test actor-critic
    print("\n3. Testing Actor-Critic...")
    action, log_prob, value = ac.get_action_and_value(states)
    print(f"   Action: {action.shape}, Log prob: {log_prob.shape}, Value: {value.shape}")
    
    # Count parameters
    actor_params = sum(p.numel() for p in actor.parameters())
    critic_params = sum(p.numel() for p in critic.parameters())
    print(f"\n4. Parameter counts:")
    print(f"   Actor: {actor_params:,} parameters")
    print(f"   Critic: {critic_params:,} parameters")
    print(f"   Total: {actor_params + critic_params:,} parameters")
    
    print("\nAll tests passed!")

